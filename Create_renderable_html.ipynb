{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create renderable html text\n",
    "\n",
    "* Get archives from common crawl and extract full html text\n",
    "* Extract relevant html parts: css-links style tags and original table code\n",
    "* Get Wayback Machine Internet Archive Link to access archived CSS files\n",
    "* Create renderable html text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gs as gs\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import imgkit\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import boto3 \n",
    "import botocore\n",
    "from bs4 import BeautifulSoup, Doctype\n",
    "from warcio import ArchiveIterator\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(r'C:\\Users\\babet\\Documents\\Studi\\Master Thesis\\Table_detection')\n",
    "\n",
    "#load gold standard dataset\n",
    "\n",
    "df = gs.load_gs(r\"C:\\Users\\babet\\Documents\\Studi\\Master Thesis\\Table_detection\\Data\\mannheim.db\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf=df\n",
    "#sdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Access Common Crawl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#delete old bucket name [commoncrawl]\n",
    "sdf.s3Link= sdf.s3Link.str.replace(r\"common-crawl/\", r\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import credentials for aws server\n",
    "with open('credentials.json') as creds:    \n",
    " credentials = json.load(creds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use boto3\n",
    "session = boto3.Session(\n",
    "    aws_access_key_id=credentials['aws_access_key'],\n",
    "    aws_secret_access_key=credentials['aws_secret_key'],\n",
    ")\n",
    "# define Amazon S3\n",
    "s3 = session.resource('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "html_offset= []\n",
    "\n",
    "for x in sdf.index:\n",
    "        \n",
    "    #assign variables\n",
    "    key= sdf.s3Link.iloc[x]\n",
    "    ra='bytes={}-{}'.format(sdf.recordOffset.iloc[x], str(int(sdf.recordEndOffset.iloc[x])+100000))\n",
    "    url= sdf.url.iloc[x]\n",
    "\n",
    "    #Get html common crawl using offsets \n",
    "    obj = s3.Object(bucket_name='commoncrawl', key=key)\n",
    "    response = obj.get(Range=ra)\n",
    "    data = response['Body']\n",
    "\n",
    "    for record in ArchiveIterator(data):\n",
    "          if record.rec_type == 'response':\n",
    "                if record.rec_headers.get_header('WARC-Target-URI') == url:\n",
    "                #f record.http_headers.get_header('Content-Type') == 'text/html':\n",
    "                    #print(record.rec_headers.get_header('WARC-Target-URI')) \n",
    "                    html_offset.append(record.content_stream().read())\n",
    "                    #print(html)\n",
    "                    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sdf['fullHtmlCode'] = html_offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('database or disk is full',)).History will not be written to the database.\n"
     ]
    }
   ],
   "source": [
    "#External disk\n",
    "os.chdir(r'E:\\Babette\\MasterThesis')\n",
    "sdf.to_pickle('gs_log.pkl')  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract relevant html parts\n",
    "\n",
    "* Extract links to css files\n",
    "* Complete relative links\n",
    "* Extract style tags\n",
    "* Extract full table code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    }
   ],
   "source": [
    "html= []\n",
    "\n",
    "for x in sdf.index:\n",
    "    #html.append(BeautifulSoup(html_offset[x], 'html.parser'))\n",
    "    html.append(BeautifulSoup(sdf.fullHtmlCode.iloc[x], 'html.parser'))\n",
    "    #print(html.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "links= []\n",
    "\n",
    "for i in html:\n",
    "    #find all css links\n",
    "    for head in i.find_all(\"head\"):\n",
    "        link= head.find_all('link')\n",
    "    longString=str()\n",
    "    for x in link:\n",
    "        longString = longString + \" \\n \"+ str(x)\n",
    "    links.append(longString)\n",
    "    #links  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sdf['links'] = links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "style= []\n",
    "\n",
    "for i in html:\n",
    "    #find all style tags\n",
    "    for head in i.find_all(\"head\"):\n",
    "        sty= head.find_all('style')\n",
    "    longString=str()\n",
    "    for x in sty:\n",
    "        longString = longString + \" \\n \"+ str(x)\n",
    "    style.append(longString)\n",
    "    #style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#create new collumn\n",
    "sdf['styleTag'] = style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#External disk\n",
    "os.chdir(r'E:\\Babette\\MasterThesis')\n",
    "sdf.to_pickle('gs_log.pkl')  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fullTable= []\n",
    "error= [1705, 4054,4362] #find out why!\n",
    "for i in html:\n",
    "    x= html.index(i)\n",
    "    if x in error:\n",
    "        fullTable.append(\"NAN\")\n",
    "    else:        \n",
    "        #print(x)\n",
    "        tables= list()\n",
    "        for tag in i.find_all(re.compile(\"table\")):\n",
    "            tables.append(tag)\n",
    "        fullTable.append(str(tables[int(sdf.tableNum.iloc[x])]))\n",
    "    #fullTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#create new collumn\n",
    "sdf['fullTable'] = fullTable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create full renderable html Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "renderCode =[]\n",
    "\n",
    "for x in sdf.index:\n",
    "    links= sdf.links.iloc[x]\n",
    "    styleTag= sdf.styleTag.iloc[x]\n",
    "    if sdf.fullTable.iloc[x] ==\"NAN\":\n",
    "        fullTable= \"<table>\" +sdf.htmlCode.iloc[x]+ \"</table\"\n",
    "    else:\n",
    "        fullTable= sdf.fullTable.iloc[x]\n",
    "    \n",
    "    #Create Table html code including links & style info\n",
    "    htmlAdd= r'<html> <head>'+ links + styleTag + ' </head> <body>  '+ fullTable + r' </body> </html>'\n",
    "    \n",
    "    renderCode.append(htmlAdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create new collumn\n",
    "sdf['renderCode'] = renderCode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#create complete links if necessary\n",
    "links_comp= []\n",
    "\n",
    "for x in sdf.index:\n",
    "    #url= sdf.url.iloc[x]\n",
    "    url= re.sub(r'/$', r'', sdf.url.iloc[x])\n",
    "    #print(url)\n",
    "    \n",
    "    #case 1 current directory\n",
    "    links1=re.sub('href=\"(?!(/|\\.\\.|http))', r'href=\"'+ url+ r'/', sdf.renderCode.iloc[x])\n",
    "\n",
    "    #case 2: current root\n",
    "    root_url= re.match('https?://(?:.*\\.)*(.+\\..+?)/', sdf.url.iloc[x]).group(0)\n",
    "    #short_url= re.sub(r\"/(?:.(?!/))+$\", r\"/\", url)\n",
    "    #print(short_url)\n",
    "    links2=re.sub(r'href=\"/(?!/)', r'href=\"'+ root_url, links1)\n",
    "\n",
    "    #case 3: two above\n",
    "    shorter_url= re.sub(r\"/[^/]*/(?:.(?!/))+$\", r\"/\", url)\n",
    "    links3=re.sub(r'href=\"../', r'href=\"'+ shorter_url, links2)\n",
    "    #print(shorter_url)\n",
    "    \n",
    "    #case4: autonomous link //\n",
    "    links4=re.sub(r'href=\"//', r'href=\"http://', links3)\n",
    "\n",
    "    \n",
    "    #same for style links\n",
    "    \n",
    "    #case 1 current directory\n",
    "    links5=re.sub('url\\((\\\\\\')(?!(/|\\.\\.|\\\"http|http|https|\\\"https))', r'url(\\''+ url+ r'/', links4)\n",
    "    links6=re.sub('url\\((?!(/|\\.\\.|\\\"http|http|https|\\\"https|\\\\\\'))', r'url('+ url+ r'/', links5)\n",
    "\n",
    "    #case 2: current root\n",
    "    #short_url= re.sub(r\"/(?:.(?!/))+$\", r\"/\", url)\n",
    "    #print(short_url)\n",
    "    links7=re.sub('url\\(\\\\*\\'*/(?!/)', r'url(\\''+ root_url, links6)\n",
    "    links8=re.sub('url\\(/(?!/)', r'url('+ root_url, links7)\n",
    "\n",
    "    #case 3: two above\n",
    "    shorter_url= re.sub(r\"/[^/]*/(?:.(?!/))+$\", r\"/\", url)\n",
    "    links9=re.sub('url\\(\\\\*\\'*\\.\\./', r'url(\\''+ shorter_url, links8)\n",
    "    links10=re.sub('url\\(\\.\\./', r'url('+ shorter_url, links9)\n",
    "    #print(shorter_url)\n",
    "    \n",
    "    #case 4 autonomous link\n",
    "    links11=re.sub('url\\(//', r'url(http://', links10)\n",
    "    \n",
    "    #caseX scr\n",
    "    links12=re.sub('src=\\\"(?!(/|\\.\\.|\\\"http|http|https|\\\"https|\\\\\\'))', r'src=\"'+ url+ r'/', links11)\n",
    "    links13=re.sub('src=\\\"/(?!/)', r'src=\"'+ root_url, links12)\n",
    "    links14=re.sub('src=\\\"\\.\\./', r'src=\"'+ shorter_url, links13)\n",
    "    links15=re.sub('src=\\\"//', r'src=\"http://', links14)\n",
    "    \n",
    "    \n",
    "    #links6=links3\n",
    "    links_comp.append(links15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf['renderCodeLink'] = links_comp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get wayback machine path to access css files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "archivePath = []\n",
    "error=[2990] #find out why!\n",
    "\n",
    "for x in sdf.index:    \n",
    "    print(x)\n",
    "    req_string= \"http://web.archive.org/wayback/available?url=\"+ str(sdf.url.iloc[x]) + \"&timestamp=20140101\"\n",
    "    contents = urllib.request.urlopen(req_string).read()\n",
    "    #print(contents)\n",
    "    dic = json.loads(contents)\n",
    "    if x in error:\n",
    "        archivePath.append(\"NAN\")\n",
    "    elif 'closest' in dic['archived_snapshots']:\n",
    "        archive_url =dic['archived_snapshots']['closest']['url']\n",
    "        #archive = re.sub(sdf.url.iloc[x], r'', archive_url)\n",
    "        archive = re.match(r'http://web.archive.org/web/\\d*/', archive_url )\n",
    "        #print(archive.group(0))\n",
    "        archivePath.append(str(archive.group(0)))\n",
    "\n",
    "        \n",
    "    else:\n",
    "        archivePath.append(\"NAN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create new collumn\n",
    "sdf['archivePath'] = archivePath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add paths to archived css documents to renderable Code\n",
    "renderCodeImage= []\n",
    "\n",
    "for x in sdf.index:\n",
    "    archivePath= sdf.archivePath.iloc[x]\n",
    "    html = sdf.renderCodeLink.iloc[x]\n",
    "    \n",
    "    if archivePath!= 'NAN':\n",
    "        htmlA=re.sub(r'\"http:', r'\"'+ archivePath+ 'http:', html)\n",
    "        htmlArchive=re.sub('\\(http:', '\\('+ archivePath+ 'http:', htmlA)\n",
    "    else:\n",
    "        htmlArchive=html\n",
    "    renderCodeImage.append(htmlArchive)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sdf['renderCodeImage']= renderCodeImage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.to_pickle('gs_renderable.pkl')  \n",
    "\n",
    "#ssdf = pd.read_pickle('gs_renderable_subset.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     5777\n",
       "unique    2935\n",
       "top        NAN\n",
       "freq      2820\n",
       "Name: archivePath, dtype: object"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf.archivePath.describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
